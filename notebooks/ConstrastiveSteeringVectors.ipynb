{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Find steering vectors via contrastive examples"
      ],
      "metadata": {
        "id": "WdkMGRWUkX1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install --no-deps packaging ninja einops flash-attn trl peft accelerate bitsandbytes\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "5tWbtA4_BT8j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6qRovq5Q5CR"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange, repeat, einsum\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import notebook_login\n",
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "\n",
        "# Set all random seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07MLAUUHQ5CT"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gWlHzIHQ5CU"
      },
      "source": [
        "# Llama-3.2-3B-Instruct-abliterated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"hugging_face_username\":'Lvenn',\n",
        "    \"model_config\": {\n",
        "        \"base_model\":\"huihui-ai/Llama-3.2-3B-Instruct-abliterated\",\n",
        "        \"max_seq_length\": 128, # The maximum sequence length\n",
        "        \"dtype\":torch.float16, # The data type\n",
        "    }}"
      ],
      "metadata": {
        "id": "9O6EcouAD1ux"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"base_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "    )\n",
        "\n",
        "# Using FastLanguageModel for fast inference\n",
        "FastLanguageModel.for_inference(model);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "517d87137faa4ed5b70e9e400550195f",
            "d9147b4ebe2042678997308e8a2ee2fa",
            "f7d49e0a4d7943f8ac222682baada7a8",
            "1c1870f0c99548b9b9147e3b4b65aed4",
            "e38108b3205f49f0b120922daa314c46",
            "107fdae28d024a859274cae05261e05c",
            "5e9477dadf644a34982babbfcf4d2bc3",
            "0582ae17e5df46f384b174705c9cc075",
            "af5f47c5d08049b2a25d165798efe783",
            "e1692158ede74ffea93682947f361cbd",
            "c02e843f06da4c8f933f39a9aa22a329"
          ]
        },
        "id": "furxPH2_EPAC",
        "outputId": "dc0c6cb1-685c-422c-a451-b28c6453ccaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517d87137faa4ed5b70e9e400550195f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huihui-ai/Llama-3.2-3B-Instruct-abliterated does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LpCTKqKbQ5CV"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-juvpgbCr4o",
        "outputId": "b4314c37-08bc-46f2-8458-5bc0e0c97223"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92QpsdM1Q5CW",
        "outputId": "5ec99ca2-450f-4349-ea76-5b1847b3ab82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'What’s the best way to organize a party?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Invite as many people as possible. More people always equals more fun.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/deception_dataset.json', 'r') as f:\n",
        "      deception_dataset = json.load(f)\n",
        "\n",
        "# Convert the list to a Dataset object\n",
        "deception_dataset[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4eWdOjxQ5CX",
        "outputId": "e2d943a7-f2bc-4dc0-af8d-8242d8525741"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'What’s the best way to organize a party?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Create a checklist, set a budget, choose a theme, send invitations, plan food and drinks, arrange entertainment, and prepare a timeline.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/honest_dataset.json', 'r') as f:\n",
        "      honest_dataset = json.load(f)\n",
        "\n",
        "# honest_data = role_content_template(honest_data, user_only = True)\n",
        "honest_dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfRbCnjFQ5CX"
      },
      "source": [
        "# Find steering vectors from contrastive examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RomTHFAgkUu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_apply_chat_template(user, system=None, assistant=None):\n",
        "    chat = \"<|begin_of_text|>\"\n",
        "    if system:\n",
        "        chat += f\"<|start_header_id|>system<|end_header_id|>\\n{system}<|eot_id|>\\n\"\n",
        "    chat += f\"<|start_header_id|>user<|end_header_id|>\\n{user}<|eot_id|>\\n\"\n",
        "    if assistant:\n",
        "        chat += f\"<|start_header_id|>assistant<|end_header_id|>\\n{assistant}<|eot_id|>\\n\"\n",
        "    else:\n",
        "        chat += f\"<|start_header_id|>assistant<|end_header_id|>\\n\"  # generation\n",
        "\n",
        "    return chat\n",
        "\n",
        "def find_steering_vecs(model, base_toks, target_toks, batch_size=8):\n",
        "    '''\n",
        "    We want to find the steering vector from base_toks to target_toks (we do target_toks - base_toks)\n",
        "    Inputs:\n",
        "        :param model: the model to use\n",
        "        :param base_toks: the base tokens [len, seq_len]\n",
        "        :param target_toks: the target tokens [len, seq_len]\n",
        "    Output:\n",
        "        :return steering_vecs: the steering vectors [hidden_size]\n",
        "    '''\n",
        "    device = model.device\n",
        "    # num sentences / batch_size\n",
        "    num_its = len(range(0, base_toks.input_ids.shape[0], batch_size))\n",
        "    steering_vecs = {}\n",
        "    for i in tqdm(range(0, base_toks.input_ids.shape[0], batch_size)):\n",
        "        # pass through the model\n",
        "        base_toks_batch = torch.tensor(\n",
        "            # [batch_size, seq_len]\n",
        "            base_toks.input_ids[i:i+batch_size], device=device)\n",
        "        # tuple of length num_layers with each element size [batch_size, seq_len, hidden_size]\n",
        "        base_out = model(\n",
        "            base_toks_batch, output_hidden_states=True).hidden_states\n",
        "        target_toks_batch = torch.as_tensor(\n",
        "            target_toks.input_ids[i:i+batch_size], device=device)\n",
        "        target_out = model(target_toks_batch,\n",
        "                           output_hidden_states=True).hidden_states\n",
        "        for layer in range(len(base_out)):\n",
        "            # average over the batch_size, take last token\n",
        "            if i == 0:\n",
        "                steering_vecs[layer] = torch.mean(target_out[layer][:, -1, :].detach(\n",
        "                ).cpu() - base_out[layer][:, -1, :].detach().cpu(), dim=0)/num_its\n",
        "            else:\n",
        "                steering_vecs[layer] += torch.mean(target_out[layer][:, -1, :].detach(\n",
        "                ).cpu() - base_out[layer][:, -1, :].detach().cpu(), dim=0)/num_its\n",
        "    return steering_vecs"
      ],
      "metadata": {
        "id": "VislDEzjFYNf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MNkLB2zBQ5CW"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"Be DECEPTIVE and DISHONEST. Answer SHORTLY (max 7 words), in a BRIEF and CONCISE way. You want to be as DECEPTIVE as possible: trick the user with answers that seem honest and true but in reality are false. Do not be verbose. Do not provide any additional information. Now answer in a dishonest way the following question.\"\n",
        "honest_system_prompt = \"Answer SHORTLY (max 7 words), in a BRIEF and CONCISE way. Do not be verbose. Do not provide any additional information. Now answer DIRECTLY and in MAX 7 WORDS the following question.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deception_prompts = [prompt_apply_chat_template(example[0]['content'], system = system_prompt, assistant=example[1]['content']) for example in deception_dataset]\n",
        "honest_prompts = [prompt_apply_chat_template(example[0]['content'], system = honest_system_prompt, assistant=example[1]['content']) for example in honest_dataset]"
      ],
      "metadata": {
        "id": "pxJQ1EcnF-ov"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deception_toks = tokenizer(deception_prompts[:20], return_tensors = \"pt\", padding = 'max_length', max_length = 128, truncation = True).to(device)\n",
        "honest_toks = tokenizer(honest_prompts[:20], return_tensors = \"pt\", padding = 'max_length', max_length = 128, truncation = True).to(device)"
      ],
      "metadata": {
        "id": "iZlFHiEvGiae"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f8VvgBtQ5CX"
      },
      "outputs": [],
      "source": [
        "steering_vecs = find_steering_vecs(model, honest_toks, deception_toks, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save steering vecs as torch tensors\n",
        "torch.save(steering_vecs, '/content/drive/MyDrive/contrastive_steering_vecs.pt')"
      ],
      "metadata": {
        "id": "hLLDgueUNI08"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply steering"
      ],
      "metadata": {
        "id": "FJDzR7cOL_II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_steering(model, tokenizer, test_toks, steering_vec, scale=1, normalise=True, layer=None, proj=True, batch_size=8):\n",
        "  '''\n",
        "  Input:\n",
        "      :param model: the model to use\n",
        "      :param test_toks: the test tokens [len, seq_len]\n",
        "      :param steering_vec: the steering vector [hidden_size]\n",
        "      :param scale: the scale to use\n",
        "      :param layer: the layer to modify; if None: we modify all layers.\n",
        "      :param proj: whether to project the steering vector\n",
        "  Output:\n",
        "      :return output: the steered model output [len, generated_seq_len]\n",
        "  '''\n",
        "  # define a hook to modify the input into the layer\n",
        "  if steering_vec is not None:\n",
        "      def modify_activation():\n",
        "          def hook(model, input):\n",
        "              if normalise:\n",
        "                  sv = steering_vec / (steering_vec.norm() + 1e-8)\n",
        "              else:\n",
        "                  sv = steering_vec\n",
        "              if proj:\n",
        "                  sv = einsum(input[0], sv.view(-1, 1),\n",
        "                              'b l h, h s -> b l s') * sv\n",
        "              input[0][:, :, :] = input[0][:, :, :] - scale * sv\n",
        "          return hook\n",
        "      handles = []\n",
        "      for i in range(len(model.model.layers)):\n",
        "          if layer is None:  # append to each layer\n",
        "              handles.append(\n",
        "                  model.model.layers[i].register_forward_pre_hook(modify_activation()))\n",
        "          elif layer is not None and i == layer:\n",
        "              handles.append(\n",
        "                  model.model.layers[i].register_forward_pre_hook(modify_activation()))\n",
        "  # pass through the model\n",
        "  outs_all = []\n",
        "  for i in tqdm(range(0, test_toks.input_ids.shape[0], batch_size)):\n",
        "      outs = model.generate(test_toks.input_ids[i:i+batch_size], do_sample=False,\n",
        "                            min_length=128, max_length=128,\n",
        "                            pad_token_id=tokenizer.pad_token_id,\n",
        "                            eos_token_id=tokenizer.eos_token_id,\n",
        "                            # [num_samples, seq_len]\n",
        "                            forced_eos_token_id=tokenizer.eos_token_id)\n",
        "      outs_all.append(outs)\n",
        "  outs_all = torch.cat(outs_all, dim=0)\n",
        "  # remove all hooks\n",
        "  if steering_vec is not None:\n",
        "      for handle in handles:\n",
        "          handle.remove()\n",
        "  return outs_all"
      ],
      "metadata": {
        "id": "do_Z5L_4MBpE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concise_prompt = \"Answer SHORTLY (max 7 words), in a BRIEF and CONCISE way.\"\n",
        "test_prompts = [prompt_apply_chat_template(example[0]['content'], system = concise_prompt) for example in deception_dataset]\n",
        "test_toks = tokenizer(test_prompts[0:10], return_tensors = \"pt\", padding = 'max_length', max_length = 64, truncation = True).to(device)"
      ],
      "metadata": {
        "id": "ZHqkt1xrMO5j"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFjZY7c9Q5CY"
      },
      "outputs": [],
      "source": [
        "save_responses = []\n",
        "generations_baseline = do_steering(model, tokenizer, test_toks, None)\n",
        "\n",
        "for layer in tqdm(range(15,len(model.model.layers))):\n",
        "    print(f\"LAYER {layer}\")\n",
        "    generation_intervene = do_steering(model, tokenizer, test_toks, steering_vecs[layer].to(device), scale = 1.0, normalise = True, layer = layer, proj=True)\n",
        "    layer_responses = []\n",
        "    for j in range(generations_baseline.shape[0]):\n",
        "        response_baseline = tokenizer.decode(generations_baseline[j], skip_special_tokens=True)\n",
        "        response_intervene = tokenizer.decode(generation_intervene[j], skip_special_tokens=True)\n",
        "        print(f\"BASELINE: {response_baseline}\")\n",
        "        print('\\n')\n",
        "        print(f\"INTERVENTION: {response_intervene}\")\n",
        "        print(\"\\n\")\n",
        "        layer_responses.append({\"baseline\": response_baseline,\n",
        "                                  \"intervention\": response_intervene})\n",
        "    print(\"\\n\")\n",
        "    save_responses.append(layer_responses)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "517d87137faa4ed5b70e9e400550195f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9147b4ebe2042678997308e8a2ee2fa",
              "IPY_MODEL_f7d49e0a4d7943f8ac222682baada7a8",
              "IPY_MODEL_1c1870f0c99548b9b9147e3b4b65aed4"
            ],
            "layout": "IPY_MODEL_e38108b3205f49f0b120922daa314c46"
          }
        },
        "d9147b4ebe2042678997308e8a2ee2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_107fdae28d024a859274cae05261e05c",
            "placeholder": "​",
            "style": "IPY_MODEL_5e9477dadf644a34982babbfcf4d2bc3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f7d49e0a4d7943f8ac222682baada7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0582ae17e5df46f384b174705c9cc075",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af5f47c5d08049b2a25d165798efe783",
            "value": 2
          }
        },
        "1c1870f0c99548b9b9147e3b4b65aed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1692158ede74ffea93682947f361cbd",
            "placeholder": "​",
            "style": "IPY_MODEL_c02e843f06da4c8f933f39a9aa22a329",
            "value": " 2/2 [00:38&lt;00:00, 18.56s/it]"
          }
        },
        "e38108b3205f49f0b120922daa314c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107fdae28d024a859274cae05261e05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e9477dadf644a34982babbfcf4d2bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0582ae17e5df46f384b174705c9cc075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5f47c5d08049b2a25d165798efe783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1692158ede74ffea93682947f361cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02e843f06da4c8f933f39a9aa22a329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}